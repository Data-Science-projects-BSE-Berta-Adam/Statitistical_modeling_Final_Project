---
title: "SPECTF"
output: html_document
date: "2023-11-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(glmnet)
library(sigmoid)
library(caret)
library(MASS)
library(ggplot2)
library(dplyr)
library(tidyverse) # multipurpose package developped by Posit
library(rstanarm) # Easy estimation of standard models with Bayesian methods 
#library(bayestestR) # Functions to analyze posterior distributions generated by rstranarm
library(mombf) # Bayesian model selection and Bayesian model averaging
library(ltm) #correlation for binary outcome
library(Hmisc) #describe
set.seed(1234)
```


```{r}
train_dataset <- read.table("/Users/bertacanal/Desktop/spectf+heart/SPECTF.train", sep=",", header = FALSE)
```

```{r}
variable_names <- c("OVERALL_DIAGNOSIS","F1R","F1S","F2R","F2S","F3R","F3S","F4R","F4S","F5R","F5S","F6R","F6S","F7R","F7S","F8R","F8S","F9R","F9S","F10R","F10S","F11R","F11S","F12R","F12S","F13R","F13S","F14R","F14S","F15R","F15S","F16R","F16S","F17R","F17S","F18R","F18S","F19R","F19S","F20R","F20S","F21R","F21S","F22R","F22S")
colnames(train_dataset) <- variable_names
```


```{r}
y_train <- train_dataset$OVERALL_DIAGNOSIS
x_train <- data.matrix(train_dataset[, !colnames(train_dataset) %in% c("OVERALL_DIAGNOSIS")])
# darwin_x <- scale(darwin_x)
```

```{r}
data <- data.frame(Class = factor(y_train))

ggplot(data, aes(x = Class, fill = Class)) +
  geom_bar() +
  labs(x = "Class", y = "Count") +
  ggtitle("Count of Two Classes")
```

### LASSO-CV (Via cross-validation)


```{r}
t0 <- Sys.time()
fit.lassocv <- cv.glmnet(x=as.matrix(x_train),y=y_train,family ='binomial', nfolds = 10, aplha = 1)
t1 <- Sys.time()
cat('\nTime elapsed: ')
```

```{r}
print(round(t1-t0,3))
```

```{r}
fit.lassocv
```

```{r}
plot(fit.lassocv)
```

```{r}
b.lassocv = as.vector(coef(fit.lassocv, s='lambda.min'))
sum(b.lassocv != 0)
```

### LASSO-BIC (Via BIC)


```{r}
lasso.bic.logistic <- function(y,x,extended=FALSE) {
  #Select model in LASSO path with best BIC (using LASSO regression estimates)
  #Input
  # - y: vector with response variable
  # - x: design matrix
  #
  #Output: list with the following elements
  # - coef: LASSO-estimated regression coefficient with lambda set via BIC
  # - ypred: predicted y
  # - lambda.opt: optimal value of lambda
  # - lambda: data.frame with bic and number of selected variables for each value of lambda
  require(glmnet)
  fit <- glmnet(x=x,y=y,family='binomial',alpha=1)
  pred <- predict(fit,newx=x,type='response')
  n <- length(y)
  p <- colSums(fit$beta!=0) + 1
  if (!extended){
    bic <- -2* colSums(y*log(pred)+(1-y)*log(1-pred)) + log(n)*p 
  } else {
    bic <- -2* colSums(y*log(pred)+(1-y)*log(1-pred)) + log(n)*p + 2*log(choose(ncol(x),p))
  }
  sel <- which.min(bic)
  beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
  ypred <- pred[,sel]
  ans <- list(model=fit,coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
  return(ans)
}
```

```{r}
fit.lassobic = lasso.bic.logistic(x = as.matrix(x_train), y = y_train, extended = FALSE)
b.lassobic = fit.lassobic$coef
names(fit.lassobic)
round(b.lassobic, 3)
```

```{r}
sum(b.lassobic != 0)
```

```{r}
fit.bayesreg <- modelSelection(y= y_train ,x= as.matrix(x_train), priorCoef=zellnerprior(tau=1), priorDelta=modelbbprior(1,1))
```

```{r}
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3] <- round(ci.bayesreg[,1:3], 3)  
ci.bayesreg[,4] <- round(ci.bayesreg[,4], 4)      
head(ci.bayesreg)
tail(ci.bayesreg)
ci.bayesreg
```




