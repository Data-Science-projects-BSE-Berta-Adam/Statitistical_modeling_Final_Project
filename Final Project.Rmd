---
title: "Final Project"
author: "Berta Canal & Adam Olivares"
date: "2023-11-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(glmnet)
library(sigmoid)
library(caret)
library(MASS)
library(ggplot2)
library(dplyr)
library(tidyverse) # multipurpose package developped by Posit
library(rstanarm) # Easy estimation of standard models with Bayesian methods 
#library(bayestestR) # Functions to analyze posterior distributions generated by rstranarm
library(mombf) # Bayesian model selection and Bayesian model averaging
library(ltm) #correlation for binary outcome
library(Hmisc) #describe
set.seed(1234)
```


```{r}
darwin <- read.csv("DARWIN.csv", sep = ",", header = TRUE)
```

```{r}
darwin_Y <- as.numeric(darwin$class %in% c("P")) #generate a dichotomous variable
darwin_x <- data.frame(darwin[, !colnames(darwin) %in% c("ID", "class")])
# darwin_x <- scale(darwin_x)
```

## Data exploration

Count the number of instances of each class (dependent variable). No clas imbalance:

```{r}
ggplot(darwin, aes(x = class)) +
  geom_bar() 
```


```{r}
data.frame(summary(darwin_x))
```

To deal with the problem of high dimensionality and correlation among features, we can summarize our current variables in a new set of 18 variables that contains the means of the same 18 variables observed across the 25 experiments. This is possible thanks to the structure of our data, which is organized in 450 variables that belongs to the results of 18 metrics for each of the 25 different experiments:

```{r}
# Extract substrings excluding numbers using gsub to obtain unique column names
unique_variable_names <- unique(
  
  gsub("\\d", "", names(darwin_x))
)
unique_variable_names

```

```{r}
# create a new dataframe only containing the means of results in each metric across experiments for each individual:
max_num_cols_per_variable <- 25 # number of experiments 

rowmean_df_gen <- function(input_df, unique_variable_names, max_num_cols_per_variable) {
  n_obs_individual <- nrow(input_df) #observations
  num_variable_names <- length(unique_variable_names) #number of unique metrics evaluated across experiments

    output_df <- data.frame(matrix(ncol = num_variable_names, nrow = n_obs_individual))
  colnames(output_df) <- unique_variable_names #create an empty dataframe
  
  for (variable_name in unique_variable_names){
    nm_vector <- paste0(variable_name, "", 1:max_num_cols_per_variable)
    output_df[variable_name] <- rowMeans(input_df[nm_vector], na.rm = TRUE)
  }
  return(output_df)
}

mean_darwin_x <- rowmean_df_gen(darwin_x, unique_variable_names, max_num_cols_per_variable)
print(mean_darwin_x)

```

correlation between x features. Pair-wise correlations are considered. If two variables have a high correlation, the function looks at the mean absolute correlation of each variable and removes the variable with the largest mean absolute correlation.

```{r}
correlations <- cor(mean_darwin_x)
highCorr <- findCorrelation(correlations, cutoff = .95, names = TRUE, exact = TRUE)
print(highCorr)
```

```{r}
correlations
```

The Point-Biserial Correlation Coefficient is a correlation measure of the strength of association between a continuous-level variable (ratio or interval data) and a binary variable.

```{r}
cor_test_results <- function(x_data, y_data, cutoff){
  
    coef <- sapply(x_data, function(x) biserial.cor(x, y_data)) #apply biserial.cor function
    status <- ifelse(abs(coef) >= cutoff, sprintf("above_%s", cutoff), sprintf("below_%s", cutoff))
    data.frame(
    #item = names(coef),
    biserial_correlation = matrix(coef),
    status = as.character(status), row.names = c(names(coef))
  )  
}
test <- cor_test_results(mean_darwin_x, darwin_Y, 0.2)
```

```{r}
test %>% count(status)
#test %>% biserial_correlation(status)
```

```{r}
test
```

```{r}
drop_darwin_x <- mean_darwin_x %>% select(-starts_with(c( "mean_gmrt", "gmrt_on_paper", "air_time", "mean_acc_on_paper", "mean_acc_in_air", "mean_jerk_on_paper", "mean_speed_in_air", "max_x_extension")))
```

With this summary, we can proceed to check the correlation coefficients and see if there is presence of multicolinearity in a simple way. With this information, we can go back to our original dataset and drop all the columns that belong to a given measure (remember, in each one of the 25 experiment the same 18 measures are collected:

```{r}
correlations <- cor(drop_darwin_x)
highCorr <- findCorrelation(correlations, cutoff = .80, names = TRUE, exact = FALSE) #exact = TRUE will cause the function to re-evaluate the average correlations at each step
print(highCorr)
correlations
```




low variance features

```{r}
var_test_results <- function(x_data, cutoff){
  
    coef <- sapply(x_data, function(x) var(x)) #variance
    status <- ifelse(coef >= cutoff, sprintf("above_%s", cutoff), sprintf("below_%s", cutoff))
    data.frame(
    item = names(coef),
    variance = matrix(round(coef,3)),
    status = as.character(status), row.names = c(names(coef))
  )  
}
var_test <- var_test_results(darwin_x, 0.2)

names_list_var_below_cutoff <- row.names(var_test[var_test$variance <= 0.2, ])
```

```{r}
var_test %>% count(status)
```



point biserial correlation of mean_darwin with the target variable y:

```{r}
mean_darwin_y_corr <- cor_test_results(mean_darwin_x, darwin_Y, 0.2)
```

```{r}
mean_darwin_y_corr %>% count(status)
mean_darwin_y_corr
```





Is invertibility of X^t possible in the original darwin_x possible?

```{r}
#solve.default(t(as.matrix(drop_darwin_x)) %*% as.matrix(drop_darwin_x))
```


### LASSO-CV (Via cross-validation)


```{r}
t0 <- Sys.time()
fit.lassocv <- cv.glmnet(x=as.matrix(drop_darwin_x),y=darwin_Y,family ='binomial', nfolds = 10, aplha = 1)
t1 <- Sys.time()
cat('\nTime elapsed: ')
```


```{r}
print(round(t1-t0,3))
```

```{r}
fit.lassocv
```

```{r}
plot(fit.lassocv) 
```

```{r}
b.lassocv = as.vector(coef(fit.lassocv, s='lambda.min'))
sum(b.lassocv != 0)
```

### LASSO-BIC (Via BIC)

Now the $\lambda$ is set via BIC using the function `lasso.bic` from `routines_seminar1.R`.

```{r}
lasso.bic.logistic <- function(y,x,extended=FALSE) {
  #Select model in LASSO path with best BIC (using LASSO regression estimates)
  #Input
  # - y: vector with response variable
  # - x: design matrix
  #
  #Output: list with the following elements
  # - coef: LASSO-estimated regression coefficient with lambda set via BIC
  # - ypred: predicted y
  # - lambda.opt: optimal value of lambda
  # - lambda: data.frame with bic and number of selected variables for each value of lambda
  require(glmnet)
  fit <- glmnet(x=x,y=y,family='binomial',alpha=1)
  pred <- predict(fit,newx=x,type='response')
  n <- length(y)
  p <- colSums(fit$beta!=0) + 1
  if (!extended){
    bic <- -2* colSums(y*log(pred)+(1-y)*log(1-pred)) + log(n)*p 
  } else {
    bic <- -2* colSums(y*log(pred)+(1-y)*log(1-pred)) + log(n)*p + 2*log(choose(ncol(x),p))
  }
  sel <- which.min(bic)
  beta <- c(fit$a0[sel],fit$beta[,sel]); names(beta)[1]= 'Intercept'
  ypred <- pred[,sel]
  ans <- list(model=fit,coef=beta,ypred=ypred,lambda.opt=fit$lambda[sel],lambda=data.frame(lambda=fit$lambda,bic=bic,nvars=p))
  return(ans)
}
```

To use the BIC criteria, the parameter `extended` is set to `FALSE`.

```{r}
fit.lassobic = lasso.bic.logistic(x = as.matrix(drop_darwin_x), y = darwin_Y, extended = FALSE)
b.lassobic = fit.lassobic$coef
names(fit.lassobic)
round(b.lassobic, 3)
```

The number of non-zero coefficients for LASSO-BIC corresponds to:
```{r}
sum(b.lassobic != 0)
```

```{r}
fit.bayesreg <- modelSelection(y= darwin_Y ,x= as.matrix(drop_darwin_x), priorCoef=zellnerprior(tau=1), priorDelta=modelbbprior(1,1))
```

```{r}
ci.bayesreg <- coef(fit.bayesreg)[-c(1,nrow(coef(fit.bayesreg))),]
sel.bayesreg <- ci.bayesreg[,4] > 0.5
ci.bayesreg[,1:3] <- round(ci.bayesreg[,1:3], 3)  
ci.bayesreg[,4] <- round(ci.bayesreg[,4], 4)      
head(ci.bayesreg)
tail(ci.bayesreg)
ci.bayesreg
```






